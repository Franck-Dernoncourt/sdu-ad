# SDU - Acronym Disambiguation

This repository contains training & development sets, acronym dictionary, and the evaluation scripts for acronym disambiguation task at SDU@AAA-21

# Dataset

The dataset folder contains four files:

- **diction.json**: A dictionary of the acronyms and their possible meanings. All predictions should use this dictionary to expand the ambiguous acronyms in the text.
- **train.json**: The training samples for acronym identification task. Each sample has three attributes:
  - tokens: The list of words (tokens) of the sample
  - acronym: The index of the ambiguous short-form, i.e., acronym, in the tokens.
  - expansion: The correct meaning, i.e., expansion, of the acronym in the sample. These expansions are selected from `diction.json` dictionary. 
  - id: The unique ID of the sample
- **dev.json**: The development set for acronym disambiguation task. The samples in `dev.json` have the same attributes as the samples in `train.json`.
- **predictions.json**: A sample prediction file created from `dev.json` to test the scoring script. The participants should submit the final test predictions of their model in the same format as the `predictions.json` file. Each prediction should have two attributes:
  - id: The ID of the sample (i.e., the same IDs used in the train/dev/test samples provided in `train.json`, `dev.json` and `test.json`) 
  - prediction: The correct meaning, i.e., expansion, of the acronym in the sample. These expansions should be selected from `diction.json` dictionary. 
  
  
# Evaluation

To evaluate the predictions (in the format provided in `dataset/predictions.json` file), run the following command:

`python scorere.py -g path/to/gold.json -p path/to/predictions.json`

The `path/to/gold.json` and `path/to/predictions.json` should be replaced with the real paths to the gold file (e.g., `dataset/dev.json` for evaluation on development set) and predictions file (i.e., the predictions generated by your system in the same format as `dataset/predictions.json` file). The official evaluation metrics are the macro-averaged precision, recall and F1 for correct expansion predictions. For verbose evaluation (including the micro-averaged precision, recall and F1 and also the accuracy of the predictions), use the following command:

`python scorere.py -g path/to/gold.json -p path/to/predictions.json -v`

# Submission

Submit the predictions of the model for the samples of the test set, which will be provided later, to [EasyChair](https://urldefense.com/v3/__https://easychair.org/conferences/?conf=sduaaai21__;!!C5qS4YX3!Sgxkhh2juEB5WzmclunaUhWV76hQBFnIc9fVz_658mfwcw6DvfoXu6GqUHOE3AQKYA$). Please note that the submitted prediction file should have the same format as the `datasset/predictions.json` file and use the IDs of the test samples. 

For more information see [SDU@AAAI-21](https://sites.google.com/view/sdu-aaai21/home)
